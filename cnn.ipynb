{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dietary-tiffany",
   "metadata": {},
   "source": [
    "# CNN Brain Tumor Detector by URISFOT\n",
    "The main purpose of this project was to build a CNN model that would classify if subject has a tumor or not base on MRI scan.\n",
    "\n",
    "The image data that was used for this problem is Brain MRI Images for Brain Tumor Detection. It conists of MRI scans of two classes:\n",
    "\n",
    ".NO - no tumor, encoded as 0\n",
    "\n",
    ".YES - tumor, encoded as 1\n",
    "\n",
    "## 1. Setting up the Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-ready",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-abraham",
   "metadata": {},
   "outputs": [],
   "source": [
    "#img and first convolucional layer dimensions.\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "\n",
    "#img generator vars\n",
    "batch_size = 32\n",
    "## 0.1 = 10% of all data to validation\n",
    "validation_split = 0.1\n",
    "\n",
    "#model name to save in folder\n",
    "modelName= \"model_1\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-shipping",
   "metadata": {},
   "source": [
    "## 2. Import Data\n",
    "### Load and transform MRI for input in to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './data/'\n",
    "image_generator = ImageDataGenerator(rescale=1/255, validation_split=validation_split)    \n",
    "\n",
    "train_data = image_generator.flow_from_directory(batch_size=32,\n",
    "                                                 directory=train_path,\n",
    "                                                 color_mode='grayscale',\n",
    "                                                 shuffle=True,\n",
    "                                                 target_size=(img_height, img_width), \n",
    "                                                 subset=\"training\",\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "val_data = image_generator.flow_from_directory(batch_size=32,\n",
    "                                                 directory=train_path,\n",
    "                                                 color_mode='grayscale',\n",
    "                                                 shuffle=True,\n",
    "                                                 target_size=(img_height, img_width), \n",
    "                                                 subset=\"validation\",\n",
    "                                                 class_mode='categorical')\n",
    "print(train_data.class_indices)\n",
    "class_names = train_data.class_indices\n",
    "print(len(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-recipient",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_data.classes)\n",
    "plt.title('Distribution of Image Ratios')\n",
    "plt.xlabel('Ratio Value')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-excess",
   "metadata": {},
   "source": [
    "### Show some imgs from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-copper",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    img, label = train_data.next()\n",
    "    plt.imshow(img[0])\n",
    "    \n",
    "    inv_dict = {value:key for key, value in class_names.items()}\n",
    "    plt.title(inv_dict[np.argmax(label[0])])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-imperial",
   "metadata": {},
   "source": [
    "## 4. Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-thickness",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "#\n",
    "model.add(layers.Conv2D(filters = 64, kernel_size = (6,6), \n",
    "                 activation ='relu',input_shape=(img_height, img_width, 1)))\n",
    "model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "#\n",
    "model.add(layers.Conv2D(filters = 128, kernel_size = (3,3), \n",
    "                 activation ='relu'))\n",
    "model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "#\n",
    "model.add(layers.Conv2D(filters = 128, kernel_size = (3,3), \n",
    "                 activation ='relu'))\n",
    "model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "#\n",
    "model.add(layers.Conv2D(filters = 128, kernel_size = (3,3), \n",
    "                 activation ='relu'))\n",
    "model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "# \n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation = \"relu\"))\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(layers.Dense(len(class_names),  activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-hampshire",
   "metadata": {},
   "source": [
    "## 5. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-encyclopedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# epochs to train\n",
    "epochs = 10\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "#opt = tf.keras.optimizers.Adam(learning_rate=0.00008)\n",
    "model.compile(optimizer=opt,\n",
    "            loss=tf.keras.losses.categorical_crossentropy,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "# steps_per_epoch=steps,\n",
    "history = model.fit(train_data, epochs=epochs, validation_data=val_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-burke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model performance\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs_range = range(1, len(history.epoch) + 1)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Train Set')\n",
    "plt.plot(epochs_range, val_acc, label='Val Set')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Train Set')\n",
    "plt.plot(epochs_range, val_loss, label='Val Set')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-watershed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(val_data, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-masters",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = './data_test/'\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_data = test_datagen.flow_from_directory(test_path, \n",
    "                                           target_size=(img_height, img_width), \n",
    "                                           color_mode='grayscale',\n",
    "                                           batch_size=batch_size, \n",
    "                                           class_mode='categorical',\n",
    "                                           shuffle=True,\n",
    "                                           seed=52)\n",
    "\n",
    "for _ in range(30):\n",
    "    img, label = test_data.next()   \n",
    "    prediction= model.predict(img)\n",
    "    plt.imshow(img[0])\n",
    "    inv_dict = {value:key for key, value in class_names.items()}\n",
    "    plt.title('Predic=>'+inv_dict[np.argmax(prediction[0])]+ '  Real=>'+inv_dict[np.argmax(label[0])])\n",
    "    if np.argmax(prediction[0]) != np.argmax(label[0]):\n",
    "        print(\"ERROR\")\n",
    "    else:\n",
    "        print(\"OK\")\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-carnival",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = './'+modelName+'/'\n",
    "if not os.path.exists(target_dir):\n",
    "  os.mkdir(target_dir)\n",
    "model.save(target_dir+'m.h5')\n",
    "model.save_weights(target_dir+'w.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-occasion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-recycling",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
